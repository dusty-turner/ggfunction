---
title: "ggfunction: A Grammar of Graphics for Mathematical Functions and Probability Distributions"
abstract: >
  The \pkg{ggfunction} R package extends \CRANpkg{ggplot2} by providing a principled,
  unified interface for visualizing mathematical functions, probability distributions,
  and empirical data distributions.
  It is organized around three complementary families. The first is a dimensional taxonomy
  classifying functions by their input and output dimensions: scalar functions
  $f\colon \mathbb{R} \to \mathbb{R}$, parametric curves
  $\boldsymbol{\gamma}\colon \mathbb{R} \to \mathbb{R}^2$, scalar fields
  $f\colon \mathbb{R}^2 \to \mathbb{R}$, and vector fields
  $\mathbf{F}\colon \mathbb{R}^2 \to \mathbb{R}^2$. The second family provides
  specialized geoms for theoretical probability distributions--density, cumulative
  distribution, mass, quantile, discrete cumulative distribution, discrete quantile,
  discrete survival, survival, hazard, and cumulative hazard functions--each with built-in support for
  region shading central to hypothesis testing and interval estimation. The third family
  provides geoms for empirical distributions computed from samples: the empirical CDF
  with a simultaneous Kolmogorov--Smirnov confidence band, the empirical quantile
  function with the inverted KS band, the empirical PMF as a lollipop chart, and the
  empirical cumulative hazard function with a transformed DKW confidence band.
  By embedding function evaluation, numerical integration, and validation directly into the
  \CRANpkg{ggplot2} layer system, \pkg{ggfunction} lets users move from a mathematical
  definition to a publication-quality visualization in a single, composable call.
author:
  - name: Dusty Turner
    affiliation: United States Military Academy
    address: West Point, NY
    email: dusty.s.turner@gmail.com
    orcid: 0000-0000-0000-0000
  - name: David Kahle
    affiliation: Baylor University
    address: Waco, TX
    email: david@kahle.io
    orcid: 0000-0002-9999-1558
  - name: Rodney X. Sturdivant
    affiliation: Baylor University
    address: Waco, TX
    email: rodney_sturdivant@baylor.edu
  - name: James Otto
    affiliation: Alcon Inc.
    address: Fort Worth, TX
    email: james.otto@alcon.com
    note: Early drafts of the (E)CDF stats and geoms.
date: "2026-02-21"
date_received: ~
journal:
  firstpage: 1
  lastpage: ~
slug: turner-kahle-sturdivant-ggfunction
creative_commons: CC BY
packages:
  cran:
    - ggplot2
    - ggdist
    - mosaic
    - metR
    - rlang
    - cli
    - patchwork
    - lattice
  bioc: ~
  other:
    - ggfunction
    - ggvfields
CTV: ~
output:
  rjtools::rjournal_pdf_article:
    toc: no
  rjtools::rjournal_web_article:
    self_contained: yes
    toc: no
    includes:
      in_header: resources/mathjax_macros.html
    lua-filter: parse-fig-caption.lua
    dev: svg
bibliography: RJreferences.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.width = 6,
  fig.height = 3,
  fig.align = "center",
  out.width = "85%",
  warning = FALSE,
  message = FALSE
)

options(
  ggplot2.continuous.colour = NULL,
  ggplot2.continuous.fill = NULL
)
```

# Introduction

Visualizing mathematical functions is one of the most common tasks in applied mathematics and statistics--and one of the most under-supported in the grammar of graphics. Whether an instructor is shading a rejection region under a normal curve, a researcher is inspecting a scalar field over a spatial domain, or a student is tracing a parametric spiral, the need to move quickly from a function's definition to its graphical representation arises constantly. In R, \CRANpkg{ggplot2} [@wickham2010layered; @wickham2016ggplot2] has become the dominant visualization framework, yet its native support for plotting functions remains narrow. The built-in `stat_function()` evaluates a univariate function over a range and draws it as a path--perfectly adequate for $f\colon \mathbb{R} \to \mathbb{R}$, but offering nothing for parametric curves, scalar fields, vector fields, or the rich family of functions associated with probability distributions.

Several packages address pieces of this gap. \CRANpkg{ggdist} [@kay2024ggdist] provides sophisticated distribution visualizations oriented toward Bayesian uncertainty communication. \CRANpkg{mosaic} [@pruim2017mosaic] offers `plotDist()` for pedagogical work, but it is built on the \CRANpkg{lattice} [@lattice] framework rather than the grammar of graphics. \CRANpkg{metR} provides contour and vector-field displays for meteorological data, though it requires precomputed grids rather than function objects. None of these packages offers a unified treatment of functions organized by their dimensional structure.

\pkg{ggfunction} fills this gap by extending \CRANpkg{ggplot2} with a principled taxonomy of function types, classified by the dimensions of their domain and codomain. It provides three complementary families of geoms:

1. A **dimensional taxonomy** covering four function types: $\mathbb{R} \to \mathbb{R}$, $\mathbb{R} \to \mathbb{R}^2$, $\mathbb{R}^2 \to \mathbb{R}$, and $\mathbb{R}^2 \to \mathbb{R}^2$.
2. A **probability distribution family** providing specialized layers for density, cumulative distribution, mass, quantile, discrete cumulative distribution, discrete quantile, discrete survival, survival, hazard, and cumulative hazard functions.
3. An **empirical data family** providing layers for the empirical CDF, empirical quantile function, empirical PMF, and empirical cumulative hazard function, each computed directly from a sample and accompanied by simultaneous Kolmogorov--Smirnov confidence bands.

Each layer follows the \CRANpkg{ggplot2} extension pattern: a `Stat` object evaluates the user-supplied function on a grid or sequence (or tabulates a sample), and a corresponding `Geom` renders the result. Because every \pkg{ggfunction} layer is a standard \CRANpkg{ggplot2} layer, it composes freely with the full ecosystem of scales, coordinates, themes, and facets.

The remainder of this article is organized as follows. We describe the design principles underlying the package in Section 2. Sections 3 and 4 present the dimensional taxonomy and probability distribution family, respectively, illustrating each with examples. Section 5 introduces the empirical data family. Section 6 addresses implementation details, Section 7 demonstrates composability with the broader grammar, Section 8 surveys related packages, and Section 9 concludes.

# Design principles

## A taxonomy grounded in dimension

The central organizing principle of \pkg{ggfunction} is that any function visualizable in two-dimensional space belongs to one of four classes, determined by the dimensions of its domain $m$ and codomain $n$:

\begin{equation}
\begin{aligned}
f &\colon \mathbb{R} \to \mathbb{R}        & \quad & (m = 1,\; n = 1) \\
\boldsymbol{\gamma} &\colon \mathbb{R} \to \mathbb{R}^2  & \quad & (m = 1,\; n = 2) \\
f &\colon \mathbb{R}^2 \to \mathbb{R}      & \quad & (m = 2,\; n = 1) \\
\mathbf{F} &\colon \mathbb{R}^2 \to \mathbb{R}^2 & \quad & (m = 2,\; n = 2)
\end{aligned}
(\#eq:taxonomy)
\end{equation}

Each class admits a natural visual encoding:

- **Scalar functions** ($\mathbb{R} \to \mathbb{R}$): curves in the Cartesian plane, optionally with region shading between the curve and the $x$-axis.
- **Parametric curves** ($\mathbb{R} \to \mathbb{R}^2$): directed paths with color encoding the time parameter.
- **Scalar fields** ($\mathbb{R}^2 \to \mathbb{R}$): raster heatmaps, contour lines colored by level value, or filled contour regions.
- **Vector fields** ($\mathbb{R}^2 \to \mathbb{R}^2$): short arrows at grid points (default) or streamlines computed by numerical integration of the field.

The naming convention--`geom_function_1d_1d()`, `geom_function_1d_2d()`, `geom_function_2d_1d()`, and `geom_function_2d_2d()`--encodes the dimensional signature directly, making the function's type explicit at the point of use.

## Integration with the grammar of graphics

The grammar of graphics [@wilkinson2005grammar] decomposes a graphic into orthogonal components: data, aesthetic mappings, geometric objects, statistical transformations, scales, coordinate systems, and facets. \CRANpkg{ggplot2} implements this decomposition through the `ggproto` object system, which allows new `Stat` and `Geom` classes to be defined and composed with existing infrastructure.

Every \pkg{ggfunction} layer follows this pattern. The user supplies a function object via the `fun` parameter and domain bounds via `xlim` and, where appropriate, `ylim`. The `Stat` evaluates the function on a suitable grid and returns a data frame whose columns correspond to the aesthetic variables expected by the `Geom`. Because the result is a standard \CRANpkg{ggplot2} layer, it composes with `+` alongside any other layer, scale, or theme.

A key design choice is the use of `rlang::inject()` to splice additional function arguments supplied through the `args` parameter. This allows users to parameterize functions without closures or anonymous wrappers:

```{r, eval=FALSE, echo=TRUE}
# instead of wrapping in an anonymous function:
ggplot() + geom_pdf(fun = function(x) dnorm(x, mean = 5, sd = 2), xlim = c(0, 10))

# users can write:
ggplot() + geom_pdf(fun = dnorm, xlim = c(0, 10), args = list(mean = 5, sd = 2))
```

This pattern is consistent across all \pkg{ggfunction} layers and mirrors the `args` parameter already familiar from `ggplot2::stat_function()`.

# The dimensional taxonomy

## Scalar functions: $\mathbb{R} \to \mathbb{R}$ {#sec-1d-1d}

The simplest case is a univariate function rendered as a curve in the plane. `geom_function_1d_1d()` generalizes `ggplot2::stat_function()` by adding support for region shading. The function is evaluated at $n$ equally-spaced points (default $n = 101$) over the specified `xlim`, and the resulting $(x, y)$ pairs are drawn as a path.

```{r sin-curve, echo=TRUE}
#| fig.cap: "The sine function over one full period."
library("ggfunction")

ggplot() +
  geom_function_1d_1d(fun = sin, xlim = c(0, 2 * pi))
```

The `shade_from` and `shade_to` parameters fill the region between the curve and the $x$-axis over a specified interval $[a, b]$. Boundary values are computed by linear interpolation via `stats::approxfun()`, so the shaded region aligns precisely with the requested interval even when $a$ or $b$ fall between evaluation points.

```{r shaded-cosine, echo=TRUE}
#| fig.cap: "The cosine function over $[0, 2\\pi]$ with the interval $[0, \\pi/2]$ shaded; the shaded area equals $\\int_0^{\\pi/2} \\cos(x)\\,dx = 1$."
ggplot() +
  geom_function_1d_1d(fun = cos, xlim = c(0, 2 * pi), shade_from = 0, shade_to = pi/2)
```

## Parametric curves: $\mathbb{R} \to \mathbb{R}^2$ {#sec-parametric}

A parametric curve $\boldsymbol{\gamma}(t) = (x(t),\, y(t))$ maps a scalar parameter--typically time--to a trajectory in the plane. `geom_function_1d_2d()` evaluates the user-supplied function over the range specified by `tlim` with step size `dt`, producing columns `t`, `x`, and `y`. By default, color is mapped to `after_stat(t)` to encode progression along the curve.

```{r lemniscate, echo=TRUE}
#| fig.cap: "The lemniscate of Bernoulli $\\boldsymbol{\\gamma}(t) = (\\cos t\\,/\\,(1+\\sin^2 t),\\; \\sin t\\cos t\\,/\\,(1+\\sin^2 t))$--a figure-eight curve along which the product of distances to the two foci is constant--with color encoding the parameter $t$."
lemniscate <- function(t) c(cos(t) / (1 + sin(t)^2), sin(t) * cos(t) / (1 + sin(t)^2))

ggplot() +
  geom_function_1d_2d(fun = lemniscate, tlim = c(0, 1.9 * pi), tail_point = TRUE)
```

The `tail_point` parameter adds a marker at the starting position, useful for indicating an initial condition. The `args` parameter supports parameterized curve families. Lissajous figures $\boldsymbol{\gamma}(t) = (\sin(at + \delta),\; \sin(bt))$ are a natural example: the frequency ratio $a/b$ governs the curve's topology, while a single function definition handles all cases via `args`:

```{r lissajous, echo=TRUE, fig.width=10, fig.height=4}
#| fig.cap: "Lissajous figures $\\boldsymbol{\\gamma}(t) = (\\sin(at + \\pi/2),\\,\\sin(bt))$ for three frequency ratios $a/b$. Each ratio produces a qualitatively distinct closed curve; the same function definition is reused across all three panels via the \\texttt{args} parameter."
lissajous <- function(t, a = 3, b = 2, delta = pi/2) {
  c(sin(a * t + delta), sin(b * t))
}

p1 <- ggplot() +
  geom_function_1d_2d(fun = lissajous, tlim = c(0, 1.9*pi), args = list(a = 1, b = 1)) + 
  ggtitle("a = 1, b = 1")

p2 <- ggplot() +
  geom_function_1d_2d(fun = lissajous, tlim = c(0, 1.9*pi), args = list(a = 2, b = 1)) + 
  ggtitle("a = 2, b = 1")

p3 <- ggplot() +
  geom_function_1d_2d(fun = lissajous, tlim = c(0, 1.9*pi), args = list(a = 3, b = 2)) +
  ggtitle("a = 3, b = 2")

library("patchwork")

(p1 | p2 | p3) + plot_layout(guides = "collect")
```

## Scalar fields: $\mathbb{R}^2 \to \mathbb{R}$ {#sec-scalar-fields}

A scalar field assigns a real value to each point in a two-dimensional domain. `geom_function_2d_1d()` evaluates the user-supplied function on a regular $n \times n$ grid (default $n = 50$) over `xlim` $\times$ `ylim`. The function must accept a numeric vector of length two and return a scalar; internally, a helper applies it row-wise over the grid. Three visualization modes are available through the `type` argument:

- `"raster"` (default): a heatmap with fill mapped to `after_stat(z)`.
- `"contour"`: iso-level curves with `colour` mapped to `after_stat(level)`, producing a colored legend automatically.
- `"contour_filled"`: filled regions between contour levels rendered by `ggplot2::GeomContourFilled`.

```{r sincos-raster, echo=TRUE}
#| fig.cap: "The function $f(x, y) = \\sin(x)\\cos(y)$ over $[-\\pi, \\pi]^2$ rendered as a raster heatmap."
f_sc <- function(u) {
  x <- u[1]; y <- u[2]
  sin(x) * cos(y)
}

ggplot() +
  geom_function_2d_1d(fun = f_sc, xlim = c(-pi, pi), ylim = c(-pi, pi))
```

The contour modes are selected simply by changing `type`:

```{r sincos-contour, echo=TRUE, fig.width=10, fig.height=4}
#| fig.cap: "The same function rendered as contour lines (left) and filled contours (right)."
p_contour <- ggplot() +
  geom_function_2d_1d(
    fun = f_sc, xlim = c(-pi, pi), ylim = c(-pi, pi), type = "contour"
  ) + ggtitle("type = 'contour'")

p_filled <- ggplot() +
  geom_function_2d_1d(
    fun = f_sc, xlim = c(-pi, pi), ylim = c(-pi, pi), type = "contour_filled"
  ) + ggtitle("type = 'contour_filled'")

p_contour | p_filled
```

For the contour variants, the `StatFunction2dContour` and `StatFunction2dContourFilled` classes extend the corresponding \CRANpkg{ggplot2} stats. The `setup_params()` method pre-computes the function grid so that `z.range` is available for automatic break computation before any data is rendered.

## Vector fields: $\mathbb{R}^2 \to \mathbb{R}^2$ {#sec-vector-fields}

A vector field $\mathbf{F}(x, y) = (F_1(x, y),\; F_2(x, y))$ assigns a direction and magnitude to each point in the plane. `geom_function_2d_2d()` delegates to \pkg{ggvfields} [@ggvfields] and supports two display modes through the `type` argument.

**Vector arrows (default).** With `type = "vector"` (the default), short arrows are drawn at each grid point, oriented according to the field direction and scaled relative to the grid spacing.

```{r rotation-field, echo=TRUE}
#| fig.cap: "The rotation field $\\mathbf{F}(x, y) = (-y, x)$ displayed as short arrows at each grid point."
f_rotation <- function(u) {
  x <- u[1]; y <- u[2]
  c(-y, x)
}

ggplot() +
  geom_function_2d_2d(fun = f_rotation, xlim = c(-1, 1), ylim = c(-1, 1))
```

**Streamlines.** With `type = "stream"`, integral curves are computed by numerical integration of the field using a fourth-order Runge--Kutta method [@ggvfields] and rendered as directed paths seeded on a regular grid.

```{r rotation-stream, echo=TRUE}
#| fig.cap: "The same rotation field rendered as streamlines, each following the counterclockwise flow induced by the field."
ggplot() +
  geom_function_2d_2d(fun = f_rotation, xlim = c(-1, 1), ylim = c(-1, 1),
    type = "stream")
```

Users whose primary interest is vector fields will find that \pkg{ggvfields} provides a richer collection of tools, including gradient fields, stream plots, and potential functions.

# The probability distribution family

Statistics education depends heavily on the visual language of probability distributions. Instructors shade tail areas to illustrate $p$-values, draw CDFs to connect probabilities with quantiles, and mark central regions to delineate confidence intervals. \pkg{ggfunction} provides a family of ten geoms that map directly onto the standard functions associated with a probability distribution, each with built-in shading support. Throughout, the user supplies an R function (e.g., `dnorm`, `pnorm`, `qnorm`) and a range; \pkg{ggfunction} handles evaluation, shading, and validation automatically.

Each geom accepts its native function type through the `fun` parameter (e.g., a PDF for `geom_pdf()`, a CDF for `geom_cdf()`), but most also accept alternate function types that are converted internally. In each case the missing characterization is derived numerically: PDFs are obtained by differentiation, CDFs by integration or interpolation, survival functions by the identity $S = 1 - F$, and quantile functions by root-finding. This cross-conversion feature lets users work with whichever characterization of a distribution is most natural--the PDF, CDF, survival function, quantile function, or hazard function--regardless of which geom they wish to plot. Exactly one function source must be provided; supplying more than one is an error. Table \@ref(tab:cross-conversion) summarizes the available routes; the `cross-conversion` vignette documents the numerical methods and expected accuracy in detail.

```{r cross-conversion, echo=FALSE}
#| tab.cap: "Cross-conversion routes for distribution geoms. Each geom accepts its native function type via \\texttt{fun} and one or more alternate types via the listed parameters."
cc <- data.frame(
  Geom = c(
    "\\texttt{geom\\_pdf()}", "\\texttt{geom\\_cdf()}",
    "\\texttt{geom\\_survival()}", "\\texttt{geom\\_qf()}",
    "\\texttt{geom\\_hf()}", "\\texttt{geom\\_chf()}",
    "\\texttt{geom\\_cdf\\_discrete()}",
    "\\texttt{geom\\_survival\\_discrete()}", "\\texttt{geom\\_qf\\_discrete()}"
  ),
  Native = c("PDF", "CDF", "Survival", "Quantile", "Hazard",
             "Cumulative hazard", "CDF", "Survival", "Quantile"),
  Alternates = c(
    "\\texttt{cdf\\_fun}, \\texttt{survival\\_fun}, \\texttt{qf\\_fun}, \\texttt{hf\\_fun}",
    "\\texttt{pdf\\_fun}, \\texttt{survival\\_fun}, \\texttt{qf\\_fun}, \\texttt{hf\\_fun}",
    "\\texttt{cdf\\_fun}, \\texttt{pdf\\_fun}, \\texttt{qf\\_fun}",
    "\\texttt{cdf\\_fun}, \\texttt{pdf\\_fun}, \\texttt{survival\\_fun}",
    "\\texttt{pdf\\_fun}, \\texttt{cdf\\_fun}, \\texttt{survival\\_fun}, \\texttt{qf\\_fun}",
    "\\texttt{hf\\_fun}, \\texttt{cdf\\_fun}, \\texttt{survival\\_fun}, \\texttt{pdf\\_fun}, \\texttt{qf\\_fun}",
    "\\texttt{pmf\\_fun}, \\texttt{survival\\_fun}",
    "\\texttt{cdf\\_fun}, \\texttt{pmf\\_fun}",
    "\\texttt{pmf\\_fun}, \\texttt{cdf\\_fun}, \\texttt{survival\\_fun}"
  ),
  check.names = FALSE
)
knitr::kable(cc, escape = FALSE, col.names = c("Geom", "Native (\\texttt{fun})", "Alternate inputs"))
```

## Density functions: `geom_pdf()`

The probability density function (PDF) of a continuous random variable $X$ satisfies

\begin{equation}
f(x) \geq 0 \quad \text{and} \quad \int_{-\infty}^{\infty} f(x)\, dx = 1.
(\#eq:pdf)
\end{equation}

`geom_pdf()` evaluates a user-supplied density and renders it as a filled area with an overlaid outline. Alternatively, a CDF can be supplied via `cdf_fun` (differentiated numerically), a survival function via `survival_fun` (converted to a CDF via $F = 1 - S$ and then differentiated), a quantile function via `qf_fun` (inverted via interpolation to obtain a CDF, then differentiated), or a hazard function via `hf_fun` (converted directly using the identity $f(x) = h(x) \cdot e^{-H(x)}$). The underlying `StatPDF` validates the normalization property \@ref(eq:pdf) using `stats::integrate()`, issuing a diagnostic via \CRANpkg{cli} [@cli] if the integral departs from unity by more than a specified tolerance--a guard against accidentally passing an unnormalized function.

Four shading modes are supported, corresponding to common pedagogical operations.

**Single threshold.** The `p` parameter specifies a cumulative probability. When `lower.tail = TRUE` (the default), the region from the left boundary to the $p$-quantile is shaded, representing $P(X \leq x_p) = p$. Setting `lower.tail = FALSE` shades the complementary upper tail.

**Two-sided interval.** The `p_lower` and `p_upper` parameters define a central region--the natural representation for a $(1 - \alpha)$ confidence interval.

**Tail shading.** Setting `shade_outside = TRUE` inverts the two-sided region, shading both tails. This directly represents the rejection region of a two-sided hypothesis test at level $\alpha$.

```{r pdf-shading-modes, echo=TRUE, fig.width=10, fig.height=4}
#| fig.cap: "Three shading modes for \\texttt{geom\\_pdf()}: lower tail ($p = 0.975$, left), central 95\\% interval (center), and two-tailed rejection region at $\\alpha = 0.05$ (right)."
p1 <- ggplot() +
  geom_pdf(fun = dnorm, xlim = c(-3, 3), p = 0.975) +
  ggtitle("p = 0.975")

p2 <- ggplot() +
  geom_pdf(
    fun = dnorm, xlim = c(-3, 3),
    p_lower = 0.025, p_upper = 0.975
  ) + ggtitle("Central 95%")

p3 <- ggplot() +
  geom_pdf(
    fun = dnorm, xlim = c(-3, 3),
    p_lower = 0.025, p_upper = 0.975, shade_outside = TRUE
  ) + ggtitle(expression(alpha == 0.05 ~ "rejection region"))

p1 | p2 | p3
```

**Highest density region.** The `shade_hdr` parameter specifies a coverage probability and shades the corresponding highest density region (HDR)--the smallest subset of the domain containing the specified probability mass. For multimodal densities the HDR may be disconnected, producing multiple disjoint shaded intervals. The algorithm follows @otto2023ggdensity: evaluate the density on the grid, normalize the values to sum to one, sort in descending order, accumulate until the target coverage is reached, and shade all grid intervals at or above the resulting density threshold.

```{r pdf-hdr, echo=TRUE}
#| fig.cap: "The 80\\% highest density region of an asymmetric bimodal density ($0.6\\,\\mathcal{N}(-2,\\,0.6^2) + 0.4\\,\\mathcal{N}(2,\\,1.2^2)$), shading both modes as two disjoint intervals with more area allocated to the taller, narrower component."
f_mix <- function(x) 0.6 * dnorm(x, mean = -2, sd = 0.6) + 0.4 * dnorm(x, mean = 2, sd = 1.2)
ggplot() +
  geom_pdf(fun = f_mix, xlim = c(-5, 6), shade_hdr = 0.8)
```

The shading boundaries for the interval-based modes are determined by trapezoidal accumulation. For $n$ evaluation points $x_1, \ldots, x_n$ with density values $y_1, \ldots, y_n$, the cumulative area at the $k$-th point is

\begin{equation}
A_k = \sum_{i=1}^{k-1} \frac{y_i + y_{i+1}}{2} \, (x_{i+1} - x_i).
(\#eq:trapezoid)
\end{equation}

The normalized values $A_k / A_n$ are then compared against the specified probability thresholds to determine the shading boundaries.

## Probability mass functions: `geom_pmf()`

For a discrete random variable with integer support, the PMF $p(k) = P(X = k)$ satisfies $\sum_k p(k) = 1$. `geom_pmf()` evaluates the PMF at each integer in `xlim` and renders the result as a lollipop chart--vertical segments from the $x$-axis to the probability value, capped with points. The underlying `StatPMF` validates normalization by summing the computed probabilities and issues a warning if they depart significantly from unity. When the support is not a sequence of consecutive integers, the `support` argument accepts an explicit numeric vector that overrides `xlim`.

```{r pmf-binomial, echo=TRUE}
#| fig.cap: "The PMF of a $\\mathrm{Binomial}(10, 0.3)$ distribution, rendered as a lollipop chart."
ggplot() +
  geom_pmf(fun = dbinom, xlim = c(0, 10), args = list(size = 10, prob = 0.3))
```

The `support` argument enables distributions whose mass points are not consecutive integers. Figure~\ref{fig:pmf-clt} illustrates this with the exact distribution of the sample mean $\bar X_n = n^{-1}\sum_{i=1}^n X_i$ of $n = 10$ i.i.d.\ $\mathrm{Bernoulli}(0.3)$ draws. The support is $\{0, 0.1, 0.2, \ldots, 1\}$ and $P(\bar X_n = k/n) = \binom{n}{k}0.3^k 0.7^{n-k}$. The Central Limit Theorem approximation $\bar X_n \approx \mathcal{N}(0.3,\, 0.3 \times 0.7 / 10)$ is overlaid as a scaled density curve whose peak matches the tallest lollipop, facilitating direct visual comparison of shape without a secondary axis.

```{r pmf-clt, echo=TRUE}
#| fig.cap: "Exact distribution of the sample mean of 10 i.i.d.~Bernoulli(0.3) draws (lollipops) with the CLT normal approximation overlaid as a scaled density curve."
p <- 0.3; n <- 10
sd_mean <- sqrt(p * (1 - p) / n)
f_mean  <- function(x) dbinom(round(x * n), size = n, prob = p)
max_pmf <- max(f_mean(seq(0, 1, by = 1/n)))
scale   <- max_pmf / dnorm(p, mean = p, sd = sd_mean)
f_clt   <- function(x) scale * dnorm(x, mean = p, sd = sd_mean)

ggplot() +
  geom_pmf(fun = f_mean, support = seq(0, 1, by = 1/n)) +
  geom_pdf(fun = f_clt, xlim = c(0, 1))
```

`geom_pmf()` supports the same shading modes as `geom_pdf()`. The `p` parameter shades lollipops up to the $p$-quantile; `p_lower` and `p_upper` define a two-sided interval; `shade_outside = TRUE` inverts it; and `shade_hdr` shades the smallest set of mass points whose total probability meets or exceeds the target coverage. Unshaded lollipops are rendered in grey to preserve the distributional context. Because a discrete distribution may not achieve the exact target coverage, `shade_hdr` finds the smallest HDR with coverage $\geq$ the requested level and issues a diagnostic via \CRANpkg{cli} when the actual coverage differs from the target by more than 0.5 percentage points.

```{r pmf-shading, echo=TRUE}
#| fig.cap: "Shading modes for \\texttt{geom\\_pmf()}: the lower 80\\% by cumulative probability (left) and the 80\\% HDR of a $\\mathrm{Binomial}(10, 0.3)$ distribution (right). Unshaded lollipops are shown in grey."
p1 <- ggplot() +
  geom_pmf(fun = dbinom, xlim = c(0, 10),
    args = list(size = 10, prob = 0.5), p = 0.8) +
  ggtitle("p = 0.8")

p2 <- ggplot() +
  geom_pmf(fun = dbinom, xlim = c(0, 10),
    args = list(size = 10, prob = 0.3), shade_hdr = 0.8) +
  ggtitle("shade_hdr = 0.8")

p1 | p2
```

## Cumulative distribution and survival functions

The CDF $F(x) = P(X \leq x)$ and survival function $S(x) = 1 - F(x) = P(X > x)$ are complementary summaries of a distribution. \pkg{ggfunction} provides continuous and discrete variants of both.

**Continuous CDF (`geom_cdf()`).** `geom_cdf()` evaluates a user-supplied CDF (e.g., `pnorm`) and renders it as a line. Alternatively, a PDF can be supplied via `pdf_fun` (integrated numerically), a survival function via `survival_fun` (converted via $F = 1 - S$), a quantile function via `qf_fun` (inverted via interpolation), or a hazard function via `hf_fun` (the cumulative hazard is integrated and then $F = 1 - e^{-H}$). It supports the same `p`, `p_lower`/`p_upper`, and `shade_outside` shading parameters as `geom_pdf()`.

```{r cdf-shaded, echo=TRUE}
#| fig.cap: "The standard normal CDF."
ggplot() +
  geom_cdf(fun = pnorm, xlim = c(-3, 3))
```

**Discrete CDF (`geom_cdf_discrete()`).** For discrete distributions, the CDF is a right-continuous step function. `geom_cdf_discrete()` accepts a CDF directly via `fun` (e.g., `pbinom`), a PMF via `pmf_fun` (from which the CDF is computed by cumulative summation), or a discrete survival function via `survival_fun` (converted via $F = 1 - S$), and renders the result with horizontal segments, dashed vertical jumps, open circles at the pre-jump value (the left limit), and closed circles at the achieved value. The `show_points` and `show_vert` parameters independently suppress the endpoint circles or vertical jump segments. The `support` argument accepts an explicit numeric vector of mass points, overriding `xlim` for distributions with non-integer or non-consecutive support.

```{r discrete-cdf, echo=TRUE}
#| fig.cap: "The discrete CDF of a $\\mathrm{Binomial}(10, 0.5)$ distribution."
ggplot() +
  geom_cdf_discrete(
    pmf_fun = dbinom, xlim = c(0, 10), args = list(size = 10, prob = 0.5)
  )
```

**Continuous survival (`geom_survival()`).** In reliability theory and biostatistics, $S(x) = 1 - F(x) = P(X > x)$ gives the probability that the event of interest has not yet occurred by time $x$. `geom_survival()` accepts a survival function directly via `fun`, or derives $S$ from a CDF via `cdf_fun` (computing $1 - F(x)$), a PDF via `pdf_fun` (integrating to obtain $F$, then computing $1 - F(x)$), or a quantile function via `qf_fun` (interpolating to obtain $F$, then computing $1 - F(x)$).

```{r survival-exp, echo=TRUE}
#| fig.cap: "The survival function of an $\\mathrm{Exponential}(0.5)$ distribution, $S(x) = e^{-0.5x}$."
ggplot() +
  geom_survival(cdf_fun = pexp, xlim = c(0, 10), args = list(rate = 0.5))
```

**Discrete survival (`geom_survival_discrete()`).** For discrete distributions, $S(x) = 1 - F(x)$ is itself a right-continuous step function. `geom_survival_discrete()` accepts a discrete survival function directly via `fun`, or derives $S$ from a CDF via `cdf_fun` or a PMF via `pmf_fun` (computing cumulative sums and then $1 - F$). It renders the result with the same visual conventions as `geom_cdf_discrete()`. The `support` argument behaves as in `geom_cdf_discrete()`.

```{r discrete-survival, echo=TRUE}
#| fig.cap: "The discrete survival function of a $\\mathrm{Binomial}(10, 0.5)$ distribution, descending from 1 to 0."
ggplot() +
  geom_survival_discrete(
    pmf_fun = dbinom, xlim = c(0, 10), args = list(size = 10, prob = 0.5)
  )
```

## Quantile functions

The quantile function $Q(p) = \inf\{x : F(x) \geq p\}$ inverts the CDF. \pkg{ggfunction} provides continuous and discrete variants.

**Continuous (`geom_qf()`).** `geom_qf()` evaluates a user-supplied quantile function (e.g., `qnorm`) over the unit interval $(0, 1)$. Alternatively, a CDF can be supplied via `cdf_fun` (inverted numerically by root-finding), a PDF via `pdf_fun` (integrated to obtain the CDF, then inverted), or a survival function via `survival_fun` (converted to a CDF via $F = 1 - S$, then inverted). Evaluation points are placed at Chebyshev nodes of the first kind, $p_k = (1 - \cos((2k-1)\pi / 2n))/2$ for $k = 1, \ldots, n$, which cluster near 0 and 1 where quantile functions are typically most curved and avoid evaluating at the exact endpoints, preventing $\pm\infty$ for unbounded distributions.

```{r qf-normal, echo=TRUE}
#| fig.cap: "The quantile function of the standard normal distribution."
ggplot() +
  geom_qf(fun = qnorm)
```

**Discrete (`geom_qf_discrete()`).** The quantile function of a discrete distribution is a left-continuous step function on $[0, 1]$. `geom_qf_discrete()` accepts a quantile function directly via `fun` (e.g., `qbinom`), or derives it from a PMF via `pmf_fun` (computing cumulative sums and inverting), a CDF via `cdf_fun` (evaluated on the integer support and inverted), or a discrete survival function via `survival_fun` (converted to a CDF via $F = 1 - S$ and then inverted). It renders the result with horizontal segments on $[0, 1]$, dashed vertical jumps, closed circles at the bottom of each jump (the value is achieved), and open circles at the top (the next value is not yet reached). The `support` argument behaves as in `geom_cdf_discrete()`.

```{r discrete-qf, echo=TRUE}
#| fig.cap: "The discrete quantile function of a $\\mathrm{Binomial}(10, 0.5)$ distribution as a left-continuous step function on $[0, 1]$."
ggplot() +
  geom_qf_discrete(
    pmf_fun = dbinom, xlim = c(0, 10), args = list(size = 10, prob = 0.5)
  )
```

## Hazard functions: `geom_hf()`

The hazard function

\begin{equation}
h(x) = \frac{f(x)}{S(x)} = \frac{f(x)}{1 - F(x)}
(\#eq:hazard)
\end{equation}

represents the instantaneous rate of failure at time $x$, conditional on survival to that point [@casella2002statistical]. `geom_hf()` accepts a hazard function directly via `fun`, or derives $h$ from a PDF and CDF supplied via `pdf_fun` and `cdf_fun`. When only one of `pdf_fun` or `cdf_fun` is provided, the missing component is derived numerically (by integration or differentiation, respectively). A survival function can also be supplied via `survival_fun` (converted to a CDF via $F = 1 - S$, then differentiated to obtain $f$), or a quantile function via `qf_fun` (interpolated to obtain $F$, then differentiated). The `args` parameter applies to all supplied functions; `pdf_args` and `cdf_args` provide overrides where the PDF and CDF require different parameterizations.

Division by $S(x)$ can cause numerical instability in the tail as $S(x) \to 0$. `StatHF` guards against this by replacing values where $S(x) \leq 0$ with `NaN`, and `GeomHF` filters these before rendering.

Three canonical hazard shapes arise in reliability modeling: decreasing (infant mortality, where early failures dominate), constant (the memoryless Exponential distribution), and increasing (wear-out failure, where older units fail at higher rates).

```{r hazard-shapes, echo=TRUE, fig.width=10, fig.height=4}
#| fig.cap: "Three canonical hazard shapes: decreasing ($\\mathrm{Weibull}(\\mathrm{shape}{=}0.5,\\,\\mathrm{scale}{=}2)$, left), constant ($\\mathrm{Exponential}(0.5)$, center), and increasing ($\\mathcal{N}(0,1)$, right)."
p_decr <- ggplot() +
  geom_hf(
    pdf_fun = dweibull, cdf_fun = pweibull,
    xlim = c(0.01, 5), args = list(shape = 0.5, scale = 2)
  ) + ggtitle("Decreasing (Weibull)")

p_flat <- ggplot() +
  geom_hf(
    pdf_fun = dexp, cdf_fun = pexp,
    xlim = c(0.01, 10), args = list(rate = 0.5)
  ) + ggtitle("Flat (Exponential)")

p_incr <- ggplot() +
  geom_hf(
    pdf_fun = dnorm, cdf_fun = pnorm,
    xlim = c(-3, 3), args = list(mean = 0, sd = 1)
  ) + ggtitle("Increasing (Normal)")

p_decr | p_flat | p_incr
```

## Cumulative hazard functions: `geom_chf()`

The cumulative hazard function

\begin{equation}
H(x) = \int_{-\infty}^{x} h(t)\, dt = -\log S(x)
(\#eq:chf)
\end{equation}

accumulates the instantaneous hazard over time. It is a monotonically non-decreasing function starting at zero that rises without bound for distributions with unbounded support. In reliability analysis, $H(x)$ is often more convenient than $h(x)$ because it is monotone and avoids the division by $S(x)$ that can cause numerical instability in the hazard function's tail.

`geom_chf()` renders $H(x)$ as a curve. It accepts a cumulative hazard function directly via `fun`, or derives $H$ from any other standard characterization: a hazard function via `hf_fun` (integrated numerically), a CDF via `cdf_fun` ($H = -\log(1 - F)$), a survival function via `survival_fun` ($H = -\log S$), a PDF via `pdf_fun` (integrated to obtain $F$, then transformed), or a quantile function via `qf_fun` (interpolated to obtain $F$, then transformed).

```{r chf-example, echo=TRUE, fig.width=10, fig.height=4}
#| fig.cap: "Cumulative hazard functions for three distributions: $\\mathrm{Exponential}(0.5)$ (left, linear), $\\mathrm{Weibull}(2, 1)$ (center, convex), and $\\mathcal{N}(0,1)$ (right, eventually superlinear). For the exponential, $H(x) = 0.5x$ confirms the constant hazard rate."
p1 <- ggplot() +
  geom_chf(cdf_fun = pexp, xlim = c(0, 10), args = list(rate = 0.5)) +
  ggtitle("Exponential(0.5)")

p2 <- ggplot() +
  geom_chf(cdf_fun = pweibull, xlim = c(0, 3), args = list(shape = 2, scale = 1)) +
  ggtitle("Weibull(2, 1)")

p3 <- ggplot() +
  geom_chf(cdf_fun = pnorm, xlim = c(-3, 3)) +
  ggtitle("Normal(0, 1)")

p1 | p2 | p3
```

# The empirical data family

Data analysis frequently requires visualizing not a theoretical distribution but the
distribution of an observed sample. \pkg{ggfunction}'s empirical data family applies
the same visual language as the probability distribution family to sample data. All
four geoms tabulate the empirical distribution via a shared helper that places mass
$c_k / n$ at each distinct observed value $x_k$, where $c_k$ is the count of
occurrences and $n$ is the total sample size; ties are handled correctly. Because the
geoms are standard \CRANpkg{ggplot2} layers, multiple groups can be overlaid by
mapping the `colour` aesthetic in the enclosing `ggplot()` call.

## Empirical CDF: `geom_ecdf()`

The empirical cumulative distribution function

\begin{equation}
\hat{F}_n(x) = \frac{1}{n}\sum_{i=1}^n \mathbf{1}(X_i \leq x)
(\#eq:ecdf)
\end{equation}

is the natural nonparametric estimate of the true CDF. `geom_ecdf()` renders
$\hat{F}_n$ as a right-continuous step function--horizontal segments, dashed vertical
jumps, and open/closed endpoint circles--matching the visual conventions of
`geom_cdf_discrete()`. When the sample is large (more than 50 distinct values), the
vertical jump segments and endpoint circles are suppressed automatically to avoid
visual clutter; this threshold can be overridden by setting `show_vert` and
`show_points` explicitly.

An optional simultaneous Kolmogorov--Smirnov confidence band is drawn around
the step function by default. The DKW inequality [@massart1990tight] is a
finite-sample result that implies

\begin{equation}
P\!\left(\sup_x |\hat{F}_n(x) - F(x)| \leq \varepsilon\right) \geq 1 - \alpha,
\quad \varepsilon = \sqrt{\frac{\log(2/\alpha)}{2n}},
(\#eq:ks-band)
\end{equation}

so the ribbon $[\hat{F}_n(x) - \varepsilon,\, \hat{F}_n(x) + \varepsilon]$ (clipped to
$[0, 1]$) covers the true CDF $F$ simultaneously at all $x$ with probability at least
$1 - \alpha$. The confidence level is controlled by the `level` parameter (default
0.95), and the ribbon's transparency by `conf_alpha` (default 0.3).

```{r ecdf-basic, echo=TRUE}
#| fig.cap: "Empirical CDFs for two groups from populations $\\mathcal{N}(0,1)$ and $\\mathcal{N}(2,1)$, with simultaneous 95\\% KS confidence bands. The bands are narrower for larger samples."
set.seed(1)
df_ecdf <- data.frame(
  x     = c(rnorm(60), rnorm(40, mean = 2)),
  group = rep(c("N(0,1), n=60", "N(2,1), n=40"), c(60, 40))
)

ggplot(df_ecdf, aes(x = x, colour = group)) +
  geom_ecdf(show_points = FALSE, show_vert = FALSE) +
  labs(x = "x", y = expression(hat(F)[n](x)), colour = NULL)
```

## Empirical quantile function: `geom_eqf()`

The empirical quantile function is the left-continuous inverse of the empirical CDF:

\begin{equation}
Q_n(p) = \inf\{x : \hat{F}_n(x) \geq p\},\quad p \in (0,1).
(\#eq:eqf)
\end{equation}

`geom_eqf()` renders $Q_n$ as a left-continuous step function on $[0,1]$ using the
same visual conventions as `geom_qf_discrete()`. The same auto-suppression rule
applies: jump segments and endpoint circles are hidden when the sample has more than
50 distinct values.

The confidence band for the quantile function follows directly from inverting
\@ref(eq:ks-band): since $\hat{F}_n(x) - \varepsilon \leq F(x) \leq \hat{F}_n(x) +
\varepsilon$ simultaneously, applying $Q$ to all three sides yields
$Q_n(p - \varepsilon) \leq Q(p) \leq Q_n(p + \varepsilon)$.

**Goodness-of-fit testing.** Because the confidence band covers the true quantile
function $Q$ with probability at least $1 - \alpha$ regardless of the underlying
distribution, overlaying a parametric quantile function $Q_0$ turns the plot into an
informal visual test: if $Q_0$ lies entirely within the band the data are consistent
with the hypothesized model; if $Q_0$ exits the band there is evidence against it. The
following example fits a normal distribution by maximum likelihood to two samples of
size $n = 100$--one genuinely normal, one exponential--and overlays the fitted normal
quantile function as a red curve.

```{r eqf-gof, echo=TRUE, fig.width=10, fig.height=4}
#| fig.cap: "Goodness-of-fit test for normality using \\texttt{geom\\_eqf()} with a 95\\% KS band. The fitted normal quantile function (red) threads through the band for the normal sample (left) but departs at both tails for the exponential sample (right), where asymmetry is most pronounced. Note that estimating parameters from the data makes the band slightly conservative (analogous to the Lilliefors correction)."
set.seed(3)

normal_qf <- function(p, x) qnorm(p, mean = mean(x), sd = sd(x))

df_normal <- data.frame("x" = rnorm(50))
p_norm <- ggplot(df_normal, aes(x = x)) +
  geom_eqf(show_points = FALSE, show_vert = FALSE) +
  geom_qf(fun  = normal_qf, args = list(x = df_normal$x), colour = "red") +
  ggtitle("Normal data")

df_exp <- data.frame("x" = rexp(50) - 1)
p_exp <- ggplot(df_exp, aes(x = x)) +
  geom_eqf(show_points = FALSE, show_vert = FALSE) +
  geom_qf(fun  = normal_qf, args = list(x = df_exp$x), colour = "red") +
  ggtitle("Exponential data")

p_norm | p_exp
```

## Empirical PMF: `geom_epmf()`

For observed data that are discrete or treated as such, the empirical PMF places
mass $c_k / n$ at each distinct observed value $x_k$ and renders the result as a
lollipop chart using the same conventions as `geom_pmf()`.

```{r epmf-grouped, echo=TRUE}
#| fig.cap: "Empirical PMFs for two groups of 40 observations each from $\\mathcal{N}(0,1)$ and $\\mathcal{N}(2,1)$, colored by group."
set.seed(2)
df_pmf <- data.frame(
  x     = round(c(rnorm(40), rnorm(40, mean = 2)), 1),
  group = rep(c("A", "B"), each = 40)
)

ggplot(df_pmf, aes(x = x, colour = group)) +
  geom_epmf() +
  labs(x = "x", y = "Empirical probability", colour = "Group")
```

## Empirical cumulative hazard: `geom_echf()`

The empirical cumulative hazard function transforms the ECDF to the hazard scale:

\begin{equation}
\hat{H}_n(x) = -\log\bigl(1 - \hat{F}_n(x)\bigr).
(\#eq:echf)
\end{equation}

At the largest observation, $\hat{F}_n(x) = 1$ and $\hat{H}_n(x) = \infty$;
this boundary is dropped automatically and the step function extends to the
right panel edge at the last finite value.

The simultaneous confidence band is obtained by applying the monotone
transformation $H = -\log(1 - F)$ to the DKW bounds \@ref(eq:ks-band). Because
the DKW inequality guarantees $\hat{F}_n(x) - \varepsilon \leq F(x) \leq
\hat{F}_n(x) + \varepsilon$ simultaneously at all $x$, applying $H = -\log(1 -
F)$ to both sides (and clipping the CDF bounds to $[0, 1)$) yields a
simultaneous band for $H(x)$ with the same coverage guarantee.

```{r echf-gof, echo=TRUE, fig.width=8, fig.height=4}
#| fig.cap: "Empirical cumulative hazard functions for simulated exponential data. An $\\text{Exp}(0.5)$ sample (left) is overlaid with the theoretical $H(x) = 0.5x$ (red line), which lies within the 95\\% confidence band. An $\\text{Exp}(1)$ sample (right) is tested against the wrong model $H(x) = 0.5x$, which departs from the band."
set.seed(5)

df_correct <- data.frame(x = rexp(80, rate = 0.5))
p_correct <- ggplot(df_correct, aes(x = x)) +
  geom_echf(show_points = FALSE, show_vert = FALSE) +
  geom_chf(cdf_fun = pexp, xlim = c(0, max(df_correct$x)),
           args = list(rate = 0.5), colour = "red") +
  labs(x = "x", y = expression(hat(H)[n](x))) +
  ggtitle("Correct model") +
  theme_minimal()

df_wrong <- data.frame(x = rexp(80, rate = 1))
p_wrong <- ggplot(df_wrong, aes(x = x)) +
  geom_echf(show_points = FALSE, show_vert = FALSE) +
  geom_chf(cdf_fun = pexp, xlim = c(0, max(df_wrong$x)),
           args = list(rate = 0.5), colour = "red") +
  labs(x = "x", y = expression(hat(H)[n](x))) +
  ggtitle("Wrong model") +
  theme_minimal()

p_correct | p_wrong
```

The theoretical cumulative hazard (red) threads through the band for the
correct model (left panel) but departs visibly for the misspecified model (right
panel), where the true rate is twice the hypothesized value.

## Validity of the confidence band

The DKW inequality \@ref(eq:ks-band) is a \emph{finite-sample} result: it
holds for every $n \geq 1$, not merely asymptotically. It guarantees that the
simultaneous confidence band contains the true CDF everywhere with probability
at least $1 - \alpha$ at any sample size. Because the KS statistic
$D_n = \sup_x |\hat{F}_n(x) - F(x)|$ has a distribution-free null distribution
for continuous $F$, the coverage guarantee holds regardless of the shape of the
true distribution.

We verify the guarantee by simulation. For each combination of sample size $n$
and nominal level $1 - \alpha$, we draw $B = 2{,}000$ independent samples from
$\mathcal{N}(0,1)$, compute $D_n$ via `ks.test()`, and record the fraction of
replications for which $D_n \leq \varepsilon_n$, where $\varepsilon_n$ is the
half-width used by `geom_ecdf()`.

```{r sim-coverage, echo=TRUE, fig.width=8, fig.height=3.5, cache=FALSE}
#| fig.cap: "Empirical simultaneous coverage of the KS confidence band over 10,000 simulations from $\\mathcal{N}(0,1)$. Dashed lines show the nominal levels; solid curves show empirical coverage. Coverage is everywhere at or above nominal for all $n$, confirming the finite-sample validity of the DKW bound \\citep{massart1990tight}. The slight conservatism at small $n$ diminishes as $n$ grows because the bound's constant is asymptotically tight."
set.seed(20240101)
B      <- 10000
ns     <- c(10, 20, 50, 100, 200, 500, 1000)
levels <- c(0.90, 0.95, 0.99)
eps_fn <- function(n, lv) sqrt(log(2 / (1 - lv)) / (2 * n))

sim_res <- do.call(rbind, lapply(ns, function(n) {
  do.call(rbind, lapply(levels, function(lv) {
    eps <- eps_fn(n, lv)
    dn  <- replicate(B,
      suppressWarnings(ks.test(rnorm(n), "pnorm", exact = FALSE)$statistic))
    data.frame(n = n, level = lv, empirical = mean(dn <= eps))
  }))
}))

ggplot(sim_res, aes(x = n, y = empirical,
                    colour = factor(level), group = factor(level))) +
  geom_hline(aes(yintercept = level, colour = factor(level)),
             linetype = "dashed", linewidth = 0.4) +
  geom_line(linewidth = 0.8) +
  geom_point(size = 2) +
  scale_x_log10(breaks = ns) +
  scale_y_continuous(
    labels = scales::percent_format(accuracy = 0.1),
    limits = c(0.88, 1.0)
  ) +
  scale_colour_manual(
    values = c("0.9" = "steelblue", "0.95" = "firebrick", "0.99" = "forestgreen"),
    labels = c("0.9" = "90%", "0.95" = "95%", "0.99" = "99%"),
    name   = "Nominal"
  ) +
  labs(x = "Sample size n (log scale)", y = "Empirical coverage") +
  theme_minimal()
```

Empirical coverage (solid lines) lies above each dashed nominal threshold
across all tested sample sizes, confirming that the DKW bound is a
\emph{finite-sample} guarantee: it holds for every $n \geq 1$, not merely in
the limit. The slight conservatism at small $n$ reflects that the bound is an
inequality; the constant $C = 2$ is tight [@massart1990tight] in the sense that
it cannot be universally reduced, and the conservatism diminishes as $n$ grows
because the KS distribution converges to the Kolmogorov distribution.

# Implementation

## Architecture

Each \pkg{ggfunction} layer is implemented as a pair of `ggproto` objects--a `Stat` and a `Geom`--wired together by a constructor function. The constructor creates a standard \CRANpkg{ggplot2} `layer()` call, threading the user-supplied function and its parameters through to the stat's `compute_group()` method. That method performs the core computation: evaluating the function on a grid or sequence and returning a data frame whose columns correspond to the aesthetic variables expected by the geom.

The separation of computation (stat) from rendering (geom) is not merely organizational. It allows users to substitute alternative geoms or compose multiple layers from the same stat. The `StatFunction2d` object, for instance, computes a grid of scalar-field values that can be rendered by `GeomRaster`, `GeomContour`, or `GeomContourFilled`, selected at construction time through the `type` parameter.

## Function injection

R's `...` mechanism does not compose well when a function is called inside a stat's `compute_group()` method, which must simultaneously receive \CRANpkg{ggplot2}-internal parameters. \pkg{ggfunction} adopts the approach used by `ggplot2::stat_function()`: additional arguments are collected in a named list (`args`) and injected at the call site via `rlang::inject()`:

```{r, eval=FALSE, echo=TRUE}
fun_injected <- function(x) {
  rlang::inject(fun(x, !!!args))
}
```

This pattern is simple, composable, and avoids the ambiguity of routing function arguments through `...`.

## Numerical considerations

Several layers involve numerical integration or accumulation:

- **PDF validation** uses `stats::integrate()` with adaptive quadrature to verify that the supplied function integrates to one over the specified domain.
- **Cross-conversion** between function types is handled by internal helpers. `pdf_to_cdf()` obtains a CDF by numerically integrating the PDF via `stats::integrate()` at each evaluation point. `cdf_to_pdf()` obtains a PDF by central finite differences of the CDF, $(F(x+h) - F(x-h)) / 2h$ with $h = 10^{-5}$. `cdf_to_qf()` inverts the CDF by root-finding via `stats::uniroot()`, with adaptive bound widening to accommodate distributions whose support is not known in advance. `hf_to_cdf()` integrates a hazard function to obtain the cumulative hazard $H(x)$ and returns $F(x) = 1 - e^{-H(x)}$. `hf_to_pdf()` uses the identity $f(x) = h(x) \cdot e^{-H(x)}$ directly, avoiding the numerical instability that would arise from chaining `hf_to_cdf` with `cdf_to_pdf`. These conversions are approximate; for distributions where analytic forms are available, supplying the native function type directly will be more accurate.
- **Shading boundaries** are computed by trapezoidal accumulation \@ref(eq:trapezoid), which is efficient and sufficient for the smooth densities encountered in practice.
- **Boundary interpolation** in `GeomFunction1d` uses `stats::approxfun()` to compute exact $y$-values at shade endpoints that fall between evaluation points.
- **Quantile function evaluation** in `geom_qf()` uses Chebyshev nodes of the first kind rather than a uniform grid, concentrating evaluation points near $p = 0$ and $p = 1$ where the function is most curved and avoiding evaluation at the exact endpoints.
- **Vector field integration** (via \pkg{ggvfields}) uses a fourth-order Runge--Kutta method with configurable step size and maximum iterations.

The default resolution of $n = 101$ for one-dimensional functions and $n = 50$ for two-dimensional grids (yielding 2500 evaluation points) balances visual fidelity against computational cost. Users may increase $n$ for functions with fine-scale structure.

# Composability and the grammar and limitations

Because every \pkg{ggfunction} layer is a standard \CRANpkg{ggplot2} layer, it composes freely with the rest of the grammar. Layers can be overlaid, scaled, themed, annotated, and faceted exactly as one would with any other geom. The following example superimposes two normal densities with different means and spreads, demonstrating that function visualization and standard \CRANpkg{ggplot2} idioms are fully interoperable:

```{r composability, echo=TRUE}
#| fig.cap: "Two normal densities with different means and spreads overlaid in a single plot, demonstrating composability with the ggplot2 grammar of graphics."
ggplot() +
  geom_pdf(
    fun = dnorm, xlim = c(-5, 8),
    args = list(mean = 0, sd = 1), alpha = 0.4
  ) +
  geom_pdf(
    fun = dnorm, xlim = c(-5, 8),
    args = list(mean = 3, sd = 1.5), alpha = 0.4
  ) +
  labs(x = "x", y = "f(x)", title = "Comparing two normal densities") +
  theme_minimal()
```

This composability is the primary advantage of embedding function visualization within the grammar of graphics, rather than providing standalone plotting functions.

While \pkg{ggfunction} composes naturally with the grammar, it is worth noting what it does \emph{not} do: it does not extend the grammar of graphics itself. In Wilkinson's formulation and its \CRANpkg{ggplot2} implementation, aesthetics are visual properties of geometric objects---position, color, size, shape, and so on. A function such as $f(x)$ is not such a property; rather, it \emph{references} aesthetics, mapping one visual dimension to another. It is therefore unclear whether functions can serve as aesthetics in any principled sense, since they occupy a different conceptual role than the data-to-visual mappings the grammar was designed around.

This distinction has practical consequences. \proglang{R} is a vectorized language, but not everything in \proglang{R} is a vector of elementary objects, and functions form the prime example. A function cannot be a column of a data frame, and it cannot be mapped within an \code{aes()} call. As a result, some grammar operations that users take for granted---faceting across levels of a variable, for instance---do not transfer directly to function visualization. \pkg{ggfunction} works within these constraints by accepting function objects as layer-level parameters rather than as mapped aesthetics, an approach that is plainly useful but inherently limited. This suggests that further research into the grammar of graphics, and correspondingly into the implementation of \CRANpkg{ggplot2} or an extension package, could enable richer treatment of functions as first-class objects in statistical visualization.

# Related packages {#sec:related}

Several existing packages overlap with \pkg{ggfunction} in purpose, and understanding their scope helps clarify where \pkg{ggfunction} fits.

1. \CRANpkg{ggplot2} [@wickham2016ggplot2]. The `stat_function()` layer handles scalar functions $\mathbb{R} \to \mathbb{R}$ competently, and `geom_function_1d_1d()` builds directly on this foundation. \CRANpkg{ggplot2} provides no analogues for parametric curves, scalar fields, vector fields, or probability-specific shading operations.

2. \CRANpkg{ggdist} [@kay2024ggdist]. This package excels at visualizing distributional uncertainty in Bayesian workflows: posterior distributions, predictive intervals, and uncertainty estimates from fitted models. Its geoms work primarily with samples or distributional objects rather than the `d`/`p`/`q`/`h` function families that \pkg{ggfunction} targets.

3. \CRANpkg{ggdensity} [@otto2023ggdensity]. This package improves bivariate density visualization in \CRANpkg{ggplot2}, with a particular focus on highest density regions (HDRs). \pkg{ggfunction}'s `shade_hdr` parameter in `geom_pdf()` follows the HDR computation approach introduced there: normalizing density values, sorting in descending order, and accumulating until the target coverage is reached.

4. \CRANpkg{mosaic} [@pruim2017mosaic]. The `plotDist()` function provides quick plots of named distributions and supports shading, but it is built on the \CRANpkg{lattice} [@lattice] graphics framework rather than \CRANpkg{ggplot2}, and therefore cannot be composed with \CRANpkg{ggplot2} layers.

5. \CRANpkg{metR} [@metR]. This package provides contour and vector-field geoms designed for meteorological data, requiring precomputed grids as inputs rather than function objects. Users who already have gridded data may find \CRANpkg{metR} more convenient; users who want to visualize a function directly will find \pkg{ggfunction} more natural.

6. \pkg{ggvfields} [@ggvfields]. \pkg{ggfunction}'s `geom_function_2d_2d()` delegates to \pkg{ggvfields} for streamline computation. For users whose primary interest is vector fields--including stream plots, gradient fields, and potential functions--\pkg{ggvfields} offers a substantially richer collection of tools built on the same \CRANpkg{ggplot2} extension architecture.

Taken together, these packages reflect the breadth of interest in function visualization within the R ecosystem. \pkg{ggfunction} is distinguished by its unified dimensional taxonomy and its integration of the full probability distribution function family--capabilities that, to our knowledge, no other single package provides within the grammar of graphics.

# Summary

\pkg{ggfunction} extends \CRANpkg{ggplot2} with a principled framework for visualizing mathematical functions, theoretical probability distributions, and empirical data distributions. Its organizing taxonomy--classifying functions by the dimensions of their domain and codomain--yields a small, memorable API that covers the function types most commonly encountered in mathematics and statistics. The probability distribution family covers ten geoms spanning continuous and discrete distributions: density, CDF, PMF, quantile, discrete CDF, discrete quantile, discrete survival, survival, hazard, and cumulative hazard functions. Each supports the shading operations central to statistical pedagogy---marking quantiles, delineating confidence regions, highlighting rejection areas, and shading highest density regions. The empirical data family complements these theoretical geoms by computing the empirical CDF, quantile function, PMF, and cumulative hazard function directly from samples, each paired with a simultaneous Kolmogorov--Smirnov confidence band derived from the DKW inequality.

By implementing each layer as a `ggproto` stat--geom pair, the package inherits the full composability of the grammar of graphics. Functions and empirical summaries can be overlaid, themed, faceted, and annotated using the same tools that \CRANpkg{ggplot2} users already know. The `args` injection pattern provides a clean interface for parameterized function families, and built-in validation catches common errors--such as unnormalized densities--before they produce misleading graphics.

\pkg{ggfunction} is available at \url{https://github.com/dusty-turner/ggfunction} and can be installed via `pak::pak("dusty-turner/ggfunction")`.



# Acknowledgments

This package and manuscript were made possible by agentic coding platforms, primarily OpenAI's Codex (GPT 5.3 Codex Extra High) and Anthropic's Claude (Sonneta and Opus 4.6). Early drafts of stats and geoms associated with CDFs, in particular the empirical CDF, were done by James Otto and were his contribution to the work. The package and manuscript were written by Dusty Turner and David Kahle, and early reflections on the design of the package were provided by Rodney Sturdivant.




```{r, echo=FALSE, results="asis"}
pkg_list <- c("ggplot2", "rlang", "cli")
for (pkg in pkg_list) {
  if (requireNamespace(pkg, quietly = TRUE)) invisible(NULL)
}
```
